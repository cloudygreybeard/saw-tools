# Analyzing Kubernetes API Usage with `oc adm must-gather` and `oc dev_tool audit`

## Overview

This guide walks Site Reliability Engineers (SREs) through auditing and analyzing Kubernetes (OpenShift) clusters using:

- `oc adm must-gather` with the `gather_audit_logs` script
- [`kubectl-dev_tool audit`](https://github.com/openshift/cluster-debug-tools)

This workflow helps identify high-volume API consumers, frequent `get` requests, or potentially suboptimal controller behaviour.

## Goals

- Extract and analyze Kubernetes API audit logs from a must-gather tarball
- Filter audit events for specific verbs and resources
- Support proactive investigations and RCA (root cause analysis)

---

## Step 1: Gather the Audit Logs

Run the following to collect audit logs from the cluster:

```bash
oc adm must-gather -- /usr/bin/gather_audit_logs
```

This will produce a folder similar to:

```
must-gather.local.*/quay-io-openshift-release-dev-ocp-v4-0*/audit_logs/kube-apiserver/
```

Each file corresponds to one API server node's logs.

> [!Note]
> **Why audit logs?** Audit logs provide a timestamped, structured trail of all API requests received by the Kubernetes API server, enabling detailed investigation of what happened, when, and by whom.

---

## Step 2: Analyze Audit Logs with `kubectl-dev_tool`

Install [`kubectl-dev_tool`](https://github.com/openshift/cluster-debug-tools) if you haven't already. Then, find the audit_logs directory obtained by the must-gather, and run an analysis command:

```bash
cd must-gather.local.*/quay-io-openshift-release-dev-ocp-v4-0*

oc dev_tool audit -f 'audit_logs/kube-apiserver' \
  --verb=get \
  --resource='*.*' \
  --resource='-subjectaccessreviews.*' \
  --resource='-tokenreviews.*'
```

### Explanation

- `--verb=get`: Focus on high-volume `get` requests, which are often generated by controllers or overzealous watchers.
- `--resource='*.*'`: Match all resource types.
- `--resource='-subjectaccessreviews.*'` & `--resource='-tokenreviews.*'`: Exclude common noisy SAR and token review calls.

> [!Note]
> **Why filter like this?** The goal is to highlight API usage patterns from workloads and controllers rather than authentication or RBAC-related queries.

#### More examples

```bash
oc dev_tool audit -f 'audit_logs/kube-apiserver' --by=user -otop
```

Example output:
```
count: 4416909, first: 2025-04-21T02:39:46+10:00, last: 2025-04-23T15:53:31+10:00, duration: 61h13m45.275034s
955229x              system:serviceaccount:openshift-cluster-version:default
552672x              system:serviceaccount:openshift-operator-lifecycle-manager:olm-operator-serviceaccount
220223x              system:serviceaccount:openshift-apiserver:openshift-apiserver-sa
191054x              system:serviceaccount:openshift-machine-config-operator:machine-config-operator
135253x              system:node:sre-shared-cluster-rando-master-0
131153x              system:apiserver
123468x              system:serviceaccount:openshift-authentication-operator:authentication-operator
112220x              system:serviceaccount:openshift-apiserver-operator:openshift-apiserver-operator
98050x               system:kube-controller-manager
93606x               system:node:sre-shared-cluster-rando-worker-eastus1-v5-rando
```

```bash
oc dev_tool audit -f 'audit_logs/kube-apiserver' --by=resource -otop
```

Example output:
```
count: 4416909, first: 2025-04-21T02:39:46+10:00, last: 2025-04-23T15:53:31+10:00, duration: 61h13m45.275034s
496348x              coordination.k8s.io/leases
446384x              operators.coreos.com/clusterserviceversions
373678x              v1/configmaps
308110x              authorization.k8s.io/v1/subjectaccessreviews
200818x              v1/secrets
170067x              rbac.authorization.k8s.io/v1/clusterrolebindings
168752x              rbac.authorization.k8s.io/rolebindings
164444x              v1/serviceaccounts
159990x              rbac.authorization.k8s.io/v1/clusterroles
138069x              apiextensions.k8s.io/v1/customresourcedefinitions
```

---

## Step 3: Interpret the Results

The tool summarizes high-volume users, frequent requests, and possible overuse patterns. Look for:

- **Repeated calls from the same `userAgent`**: Might indicate a misbehaving operator or controller.
- **High frequency from a specific `namespace` or `serviceAccount`**: Could be an over-polling app.
- **Calls without RBAC errors but excessive frequency**: Opportunities for tuning.

> [!Tip]
> - Combine with Prometheus metrics (e.g., `apiserver_request_total`) to cross-check traffic volume.
> - Use timestamps to correlate with incident timelines.
> - Feed findings into tuning efforts: adjust controller resync periods, optimize watches, or consolidate frequent API calls.

---

## Cleanup

Audit logs can be large. After analysis, clean up temporary data:

```bash
rm -rf must-gather.local.*
```

---

## References

- [kubectl-dev_tool GitHub](https://github.com/openshift/cluster-debug-tools)
- [Must-gather documentation](https://docs.openshift.com/container-platform/latest/support/gathering-cluster-data.html)
- ["What is overloading my API?", from RedHatQuickCourses RH1 Lab 16 Module 4](https://github.com/RedHatQuickCourses/rh1-lab16-must-gather/blob/main/content/modules/ROOT/pages/module-04.adoc)
